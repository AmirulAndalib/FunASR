<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Speech Recognition &mdash; FunASR  documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Punctuation Restoration" href="punc_recipe.html" />
    <link rel="prev" title="Docker" href="../installation/docker.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            FunASR
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation/docker.html">Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Academic Egs</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Speech Recognition</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#quick-start">Quick Start</a></li>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#stage-0-data-preparation">Stage 0: Data preparation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stage-1-feature-and-cmvn-generation">Stage 1: Feature and CMVN Generation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stage-2-dictionary-preparation">Stage 2: Dictionary Preparation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stage-3-lm-training">Stage 3: LM Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stage-4-asr-training">Stage 4: ASR Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="#stage-5-decoding">Stage 5: Decoding</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#change-settings">Change settings</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="punc_recipe.html">Punctuation Restoration</a></li>
<li class="toctree-l1"><a class="reference internal" href="vad_recipe.html">Voice Activity Detection</a></li>
<li class="toctree-l1"><a class="reference internal" href="sv_recipe.html">Speaker Verification</a></li>
<li class="toctree-l1"><a class="reference internal" href="sd_recipe.html">Speaker Diarization</a></li>
</ul>
<p class="caption"><span class="caption-text">ModelScope Egs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../modelscope_pipeline/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modelscope_pipeline/asr_pipeline.html">Speech Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modelscope_pipeline/vad_pipeline.html">Voice Activity Detection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modelscope_pipeline/punc_pipeline.html">Punctuation Restoration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modelscope_pipeline/tp_pipeline.html">Timestamp Prediction (FA)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modelscope_pipeline/sv_pipeline.html">Speaker Verification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modelscope_pipeline/sd_pipeline.html">Speaker Diarization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modelscope_pipeline/itn_pipeline.html">Inverse Text Normalization (ITN)</a></li>
</ul>
<p class="caption"><span class="caption-text">Model Zoo</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../model_zoo/modelscope_models.html">Pretrained Models on ModelScope</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_zoo/huggingface_models.html">Pretrained Models on Huggingface</a></li>
</ul>
<p class="caption"><span class="caption-text">Runtime and Service</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../runtime/export.html">Export models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime/onnxruntime_python.html">ONNXRuntime-python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime/onnxruntime_cpp.html">ONNXRuntime-cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime/libtorch_python.html">Libtorch-python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime/html5.html">Html5 server for asr service</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime/websocket_python.html">Service with websocket-python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime/websocket_cpp.html">Service with websocket-cpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime/grpc_python.html">Service with grpc-python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime/grpc_cpp.html">Service with grpc-cpp</a></li>
</ul>
<p class="caption"><span class="caption-text">Benchmark and Leadboard</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../benchmark/benchmark_onnx.html">CPU Benchmark (ONNX-python)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmark/benchmark_onnx_cpp.html">CPU Benchmark (ONNX-cpp)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../benchmark/benchmark_libtorch.html">CPU Benchmark (Libtorch)</a></li>
</ul>
<p class="caption"><span class="caption-text">Funasr Library</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../reference/build_task.html">Build custom tasks</a></li>
</ul>
<p class="caption"><span class="caption-text">Papers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../reference/papers.html">Papers</a></li>
</ul>
<p class="caption"><span class="caption-text">Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../reference/application.html">Audio Cut</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/application.html#realtime-speech-recognition">Realtime Speech Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/application.html#audio-chat">Audio Chat</a></li>
</ul>
<p class="caption"><span class="caption-text">FQA</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../reference/FQA.html">FQA</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">FunASR</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Speech Recognition</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/academic_recipe/asr_recipe.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="speech-recognition">
<h1>Speech Recognition<a class="headerlink" href="#speech-recognition" title="Permalink to this headline"></a></h1>
<p>In FunASR, we provide several ASR benchmarks, such as AISHLL, Librispeech, WenetSpeech, while different model architectures are supported, including conformer, paraformer, uniasr.</p>
<div class="section" id="quick-start">
<h2>Quick Start<a class="headerlink" href="#quick-start" title="Permalink to this headline"></a></h2>
<p>After downloaded and installed FunASR, users can use our provided recipes to easily reproduce the relevant experimental results. Here we take “paraformer on AISHELL-1” as an example.</p>
<p>First, move to the corresponding dictionary of the AISHELL-1 paraformer example.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span> egs/aishell/paraformer
</pre></div>
</div>
<p>Then you can directly start the recipe as follows:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>conda activate funasr
. ./run.sh --CUDA_VISIBLE_DEVICES<span class="o">=</span><span class="s2">&quot;0,1&quot;</span> --gpu_num<span class="o">=</span><span class="m">2</span>
</pre></div>
</div>
<p>The training log files are saved in <code class="docutils literal notranslate"><span class="pre">${exp_dir}/exp/${model_dir}/log/train.log.*</span></code>， which can be viewed using the following command:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>vim exp/*_train_*/log/train.log.0
</pre></div>
</div>
<p>Users can observe the training loss, prediction accuracy and other training information, like follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>... 1epoch:train:751-800batch:800num_updates: ... loss_ctc=106.703, loss_att=86.877, acc=0.029, loss_pre=1.552 ...
... 1epoch:train:801-850batch:850num_updates: ... loss_ctc=107.890, loss_att=87.832, acc=0.029, loss_pre=1.702 ...
</pre></div>
</div>
<p>At the end of each epoch, the evaluation metrics are calculated on the validation set, like follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>... [valid] loss_ctc=99.914, cer_ctc=1.000, loss_att=80.512, acc=0.029, cer=0.971, wer=1.000, loss_pre=1.952, loss=88.285 ...
</pre></div>
</div>
<p>Also, users can use tensorboard to observe these training information by the following command:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>tensorboard --logdir <span class="si">${</span><span class="nv">exp_dir</span><span class="si">}</span>/exp/<span class="si">${</span><span class="nv">model_dir</span><span class="si">}</span>/tensorboard/train
</pre></div>
</div>
<p>Here is an example of loss:</p>
<img src="images/loss.png" width="200"/><p>The inference results are saved in <code class="docutils literal notranslate"><span class="pre">${exp_dir}/exp/${model_dir}/decode_asr_*/$dset</span></code>. The main two files are <code class="docutils literal notranslate"><span class="pre">text.cer</span></code> and <code class="docutils literal notranslate"><span class="pre">text.cer.txt</span></code>. <code class="docutils literal notranslate"><span class="pre">text.cer</span></code> saves the comparison between the recognized text and the reference text, like follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>...
BAC009S0764W0213(nwords=11,cor=11,ins=0,del=0,sub=0) corr=100.00%,cer=0.00%
ref:    构 建 良 好 的 旅 游 市 场 环 境
res:    构 建 良 好 的 旅 游 市 场 环 境
...
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">text.cer.txt</span></code> saves the final results, like follows:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>%WER ...
%SER ...
Scored ... sentences, ...
</pre></div>
</div>
</div>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline"></a></h2>
<p>We provide a recipe <code class="docutils literal notranslate"><span class="pre">egs/aishell/paraformer/run.sh</span></code> for training a paraformer model on AISHELL-1 dataset. This recipe consists of five stages, supporting training on multiple GPUs and decoding by CPU or GPU. Before introducing each stage in detail, we first explain several parameters which should be set by users.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code>: <code class="docutils literal notranslate"><span class="pre">0,1</span></code> (Default), visible gpu list</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpu_num</span></code>: <code class="docutils literal notranslate"><span class="pre">2</span></code> (Default), the number of GPUs used for training</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpu_inference</span></code>: <code class="docutils literal notranslate"><span class="pre">true</span></code> (Default), whether to use GPUs for decoding</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">njob</span></code>: <code class="docutils literal notranslate"><span class="pre">1</span></code>  (Default),for CPU decoding, indicating the total number of CPU jobs; for GPU decoding, indicating the number of jobs on each GPU</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">raw_data</span></code>: the raw path of AISHELL-1 dataset</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">feats_dir</span></code>: the path for saving processed data</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">token_type</span></code>: <code class="docutils literal notranslate"><span class="pre">char</span></code> (Default), indicate how to process text</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">type</span></code>: <code class="docutils literal notranslate"><span class="pre">sound</span></code> (Default), set the input type</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scp</span></code>: <code class="docutils literal notranslate"><span class="pre">wav.scp</span></code> (Default), set the input file</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">nj</span></code>: <code class="docutils literal notranslate"><span class="pre">64</span></code> (Default), the number of jobs for data preparation</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">speed_perturb</span></code>: <code class="docutils literal notranslate"><span class="pre">&quot;0.9,</span> <span class="pre">1.0</span> <span class="pre">,1.1&quot;</span></code> (Default), the range of speech perturbed</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">exp_dir</span></code>: the path for saving experimental results</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tag</span></code>: <code class="docutils literal notranslate"><span class="pre">exp1</span></code> (Default), the suffix of experimental result directory</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">stage</span></code> <code class="docutils literal notranslate"><span class="pre">0</span></code> (Default), start the recipe from the specified stage</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">stop_stage</span></code> <code class="docutils literal notranslate"><span class="pre">5</span></code> (Default), stop the recipe from the specified stage</p></li>
</ul>
<div class="section" id="stage-0-data-preparation">
<h3>Stage 0: Data preparation<a class="headerlink" href="#stage-0-data-preparation" title="Permalink to this headline"></a></h3>
<p>This stage processes raw AISHELL-1 dataset <code class="docutils literal notranslate"><span class="pre">$raw_data</span></code> and generates the corresponding <code class="docutils literal notranslate"><span class="pre">wav.scp</span></code> and <code class="docutils literal notranslate"><span class="pre">text</span></code> in <code class="docutils literal notranslate"><span class="pre">$feats_dir/data/xxx</span></code>. <code class="docutils literal notranslate"><span class="pre">xxx</span></code> means <code class="docutils literal notranslate"><span class="pre">train/dev/test</span></code>. Here we assume users have already downloaded AISHELL-1 dataset. If not, users can download data <a class="reference external" href="https://www.openslr.org/33/">here</a> and set the path for <code class="docutils literal notranslate"><span class="pre">$raw_data</span></code>. The examples of <code class="docutils literal notranslate"><span class="pre">wav.scp</span></code> and <code class="docutils literal notranslate"><span class="pre">text</span></code> are as follows:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">wav.scp</span></code></p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">BAC009S0002W0122</span> <span class="o">/</span><span class="n">nfs</span><span class="o">/</span><span class="n">ASR_DATA</span><span class="o">/</span><span class="n">AISHELL</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="n">data_aishell</span><span class="o">/</span><span class="n">wav</span><span class="o">/</span><span class="n">train</span><span class="o">/</span><span class="n">S0002</span><span class="o">/</span><span class="n">BAC009S0002W0122</span><span class="o">.</span><span class="n">wav</span>
<span class="n">BAC009S0002W0123</span> <span class="o">/</span><span class="n">nfs</span><span class="o">/</span><span class="n">ASR_DATA</span><span class="o">/</span><span class="n">AISHELL</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="n">data_aishell</span><span class="o">/</span><span class="n">wav</span><span class="o">/</span><span class="n">train</span><span class="o">/</span><span class="n">S0002</span><span class="o">/</span><span class="n">BAC009S0002W0123</span><span class="o">.</span><span class="n">wav</span>
<span class="n">BAC009S0002W0124</span> <span class="o">/</span><span class="n">nfs</span><span class="o">/</span><span class="n">ASR_DATA</span><span class="o">/</span><span class="n">AISHELL</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="n">data_aishell</span><span class="o">/</span><span class="n">wav</span><span class="o">/</span><span class="n">train</span><span class="o">/</span><span class="n">S0002</span><span class="o">/</span><span class="n">BAC009S0002W0124</span><span class="o">.</span><span class="n">wav</span>
<span class="o">...</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">text</span></code></p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">BAC009S0002W0122</span> <span class="n">而</span> <span class="n">对</span> <span class="n">楼</span> <span class="n">市</span> <span class="n">成</span> <span class="n">交</span> <span class="n">抑</span> <span class="n">制</span> <span class="n">作</span> <span class="n">用</span> <span class="n">最</span> <span class="n">大</span> <span class="n">的</span> <span class="n">限</span> <span class="n">购</span>
<span class="n">BAC009S0002W0123</span> <span class="n">也</span> <span class="n">成</span> <span class="n">为</span> <span class="n">地</span> <span class="n">方</span> <span class="n">政</span> <span class="n">府</span> <span class="n">的</span> <span class="n">眼</span> <span class="n">中</span> <span class="n">钉</span>
<span class="n">BAC009S0002W0124</span> <span class="n">自</span> <span class="n">六</span> <span class="n">月</span> <span class="n">底</span> <span class="n">呼</span> <span class="n">和</span> <span class="n">浩</span> <span class="n">特</span> <span class="n">市</span> <span class="n">率</span> <span class="n">先</span> <span class="n">宣</span> <span class="n">布</span> <span class="n">取</span> <span class="n">消</span> <span class="n">限</span> <span class="n">购</span> <span class="n">后</span>
<span class="o">...</span>
</pre></div>
</div>
<p>These two files both have two columns, while the first column is wav ids and the second column is the corresponding wav paths/label tokens.</p>
</div>
<div class="section" id="stage-1-feature-and-cmvn-generation">
<h3>Stage 1: Feature and CMVN Generation<a class="headerlink" href="#stage-1-feature-and-cmvn-generation" title="Permalink to this headline"></a></h3>
<p>This stage computes CMVN based on <code class="docutils literal notranslate"><span class="pre">train</span></code> dataset, which is used in the following stages. Users can set <code class="docutils literal notranslate"><span class="pre">nj</span></code> to control the number of jobs for computing CMVN. The generated CMVN file is saved as <code class="docutils literal notranslate"><span class="pre">$feats_dir/data/train/cmvn/am.mvn</span></code>.</p>
</div>
<div class="section" id="stage-2-dictionary-preparation">
<h3>Stage 2: Dictionary Preparation<a class="headerlink" href="#stage-2-dictionary-preparation" title="Permalink to this headline"></a></h3>
<p>This stage processes the dictionary, which is used as a mapping between label characters and integer indices during ASR training. The processed dictionary file is saved as <code class="docutils literal notranslate"><span class="pre">$feats_dir/data/$lang_toekn_list/$token_type/tokens.txt</span></code>. An example of <code class="docutils literal notranslate"><span class="pre">tokens.txt</span></code> is as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">blank</span><span class="o">&gt;</span>
<span class="o">&lt;</span><span class="n">s</span><span class="o">&gt;</span>
<span class="o">&lt;/</span><span class="n">s</span><span class="o">&gt;</span>
<span class="n">一</span>
<span class="n">丁</span>
<span class="o">...</span>
<span class="n">龚</span>
<span class="n">龟</span>
<span class="o">&lt;</span><span class="n">unk</span><span class="o">&gt;</span>
</pre></div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;blank&gt;</span></code>: indicates the blank token for CTC, must be in the first line</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;s&gt;</span></code>: indicates the start-of-sentence token, must be in the second line</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;/s&gt;</span></code>: indicates the end-of-sentence token, must be in the third line</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&lt;unk&gt;</span></code>: indicates the out-of-vocabulary token, must be in the last line</p></li>
</ul>
</div>
<div class="section" id="stage-3-lm-training">
<h3>Stage 3: LM Training<a class="headerlink" href="#stage-3-lm-training" title="Permalink to this headline"></a></h3>
</div>
<div class="section" id="stage-4-asr-training">
<h3>Stage 4: ASR Training<a class="headerlink" href="#stage-4-asr-training" title="Permalink to this headline"></a></h3>
<p>This stage achieves the training of the specified model. To start training, users should manually set <code class="docutils literal notranslate"><span class="pre">exp_dir</span></code> to specify the path for saving experimental results. By default, the best <code class="docutils literal notranslate"><span class="pre">$keep_nbest_models</span></code> checkpoints on validation dataset will be averaged to generate a better model and adopted for decoding. FunASR implements <code class="docutils literal notranslate"><span class="pre">train.py</span></code> for training different models and users can configure the following parameters if necessary.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">task_name</span></code>: <code class="docutils literal notranslate"><span class="pre">asr</span></code> (Default), specify the task type of the current recipe</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gpu_num</span></code>: <code class="docutils literal notranslate"><span class="pre">2</span></code> (Default), specify the number of GPUs for training. When <code class="docutils literal notranslate"><span class="pre">gpu_num</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>, DistributedDataParallel (DDP, the detail can be found <a class="reference external" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">here</a>) training will be enabled. Correspondingly, <code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> should be set to specify which ids of GPUs will be used.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">use_preprocessor</span></code>: <code class="docutils literal notranslate"><span class="pre">true</span></code> (Default), specify whether to use pre-processing on each sample</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">token_list</span></code>: the path of token list for training</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dataset_type</span></code>: <code class="docutils literal notranslate"><span class="pre">small</span></code> (Default). FunASR supports <code class="docutils literal notranslate"><span class="pre">small</span></code> dataset type for training small datasets. Besides, an optional iterable-style DataLoader based on <a class="reference external" href="https://pytorch.org/data/beta/torchdata.datapipes.iter.html">Pytorch Iterable-style DataPipes</a> for large datasets is supported and users can specify <code class="docutils literal notranslate"><span class="pre">dataset_type=large</span></code> to enable it.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data_dir</span></code>: the path of data. Specifically, the data for training is saved in <code class="docutils literal notranslate"><span class="pre">$data_dir/data/$train_set</span></code> while the data for validation is saved in <code class="docutils literal notranslate"><span class="pre">$data_dir/data/$valid_set</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">data_file_names</span></code>: <code class="docutils literal notranslate"><span class="pre">&quot;wav.scp,text&quot;</span></code> specify the speech and text file names for ASR</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cmvn_file</span></code>: the path of cmvn file</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">resume</span></code>: <code class="docutils literal notranslate"><span class="pre">true</span></code>, whether to enable “checkpoint training”</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">config</span></code>: the path of configuration file, which is usually a YAML file in <code class="docutils literal notranslate"><span class="pre">conf</span></code> directory. In FunASR, the parameters of the training, including model, optimization, dataset, etc., can also be set in this file. Note that if the same parameters are specified in both recipe and config file, the parameters of recipe will be employed</p></li>
</ul>
</div>
<div class="section" id="stage-5-decoding">
<h3>Stage 5: Decoding<a class="headerlink" href="#stage-5-decoding" title="Permalink to this headline"></a></h3>
<p>This stage generates the recognition results and calculates the <code class="docutils literal notranslate"><span class="pre">CER</span></code> to verify the performance of the trained model.</p>
<ul class="simple">
<li><p>Mode Selection</p></li>
</ul>
<p>As we support paraformer, uniasr, conformer and other models in FunASR, a <code class="docutils literal notranslate"><span class="pre">mode</span></code> parameter should be specified as <code class="docutils literal notranslate"><span class="pre">asr/paraformer/uniasr</span></code> according to the trained model.</p>
<ul class="simple">
<li><p>Configuration</p></li>
</ul>
<p>We support CTC decoding, attention decoding and hybrid CTC-attention decoding in FunASR, which can be specified by <code class="docutils literal notranslate"><span class="pre">ctc_weight</span></code> in a YAML file in <code class="docutils literal notranslate"><span class="pre">conf</span></code> directory. Specifically, <code class="docutils literal notranslate"><span class="pre">ctc_weight=1.0</span></code> indicates CTC decoding, <code class="docutils literal notranslate"><span class="pre">ctc_weight=0.0</span></code> indicates attention decoding, <code class="docutils literal notranslate"><span class="pre">0.0&lt;ctc_weight&lt;1.0</span></code> indicates hybrid CTC-attention decoding.</p>
<ul class="simple">
<li><p>CPU/GPU Decoding</p></li>
</ul>
<p>We support CPU and GPU decoding in FunASR. For CPU decoding, you should set <code class="docutils literal notranslate"><span class="pre">gpu_inference=False</span></code> and set <code class="docutils literal notranslate"><span class="pre">njob</span></code> to specify the total number of CPU decoding jobs. For GPU decoding, you should set <code class="docutils literal notranslate"><span class="pre">gpu_inference=True</span></code>. You should also set <code class="docutils literal notranslate"><span class="pre">gpuid_list</span></code> to indicate which GPUs are used for decoding and <code class="docutils literal notranslate"><span class="pre">njobs</span></code> to indicate the number of decoding jobs on each GPU.</p>
<ul class="simple">
<li><p>Performance</p></li>
</ul>
<p>We adopt <code class="docutils literal notranslate"><span class="pre">CER</span></code> to verify the performance. The results are in <code class="docutils literal notranslate"><span class="pre">$exp_dir/exp/$model_dir/$decoding_yaml_name/$average_model_name/$dset</span></code>, namely <code class="docutils literal notranslate"><span class="pre">text.cer</span></code> and <code class="docutils literal notranslate"><span class="pre">text.cer.txt</span></code>. <code class="docutils literal notranslate"><span class="pre">text.cer</span></code> saves the comparison between the recognized text and the reference text while <code class="docutils literal notranslate"><span class="pre">text.cer.txt</span></code> saves the final <code class="docutils literal notranslate"><span class="pre">CER</span></code> results. The following is an example of <code class="docutils literal notranslate"><span class="pre">text.cer</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">...</span>
<span class="n">BAC009S0764W0213</span><span class="p">(</span><span class="n">nwords</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span><span class="n">cor</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span><span class="n">ins</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="k">del</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">sub</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="n">corr</span><span class="o">=</span><span class="mf">100.00</span><span class="o">%</span><span class="p">,</span><span class="n">cer</span><span class="o">=</span><span class="mf">0.00</span><span class="o">%</span>
<span class="n">ref</span><span class="p">:</span>    <span class="n">构</span> <span class="n">建</span> <span class="n">良</span> <span class="n">好</span> <span class="n">的</span> <span class="n">旅</span> <span class="n">游</span> <span class="n">市</span> <span class="n">场</span> <span class="n">环</span> <span class="n">境</span>
<span class="n">res</span><span class="p">:</span>    <span class="n">构</span> <span class="n">建</span> <span class="n">良</span> <span class="n">好</span> <span class="n">的</span> <span class="n">旅</span> <span class="n">游</span> <span class="n">市</span> <span class="n">场</span> <span class="n">环</span> <span class="n">境</span>
<span class="o">...</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="change-settings">
<h2>Change settings<a class="headerlink" href="#change-settings" title="Permalink to this headline"></a></h2>
<p>Here we explain how to perform common custom settings, which can help users to modify scripts according to their own needs.</p>
<ul class="simple">
<li><p>Training with specified GPUs</p></li>
</ul>
<p>For example, if users want to use 2 GPUs with id <code class="docutils literal notranslate"><span class="pre">2</span></code> and `3, users can run the following command:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>. ./run.sh --CUDA_VISIBLE_DEVICES <span class="s2">&quot;2,3&quot;</span> --gpu_num <span class="m">2</span> 
</pre></div>
</div>
<ul class="simple">
<li><p>Start from/Stop at a specified stage</p></li>
</ul>
<p>The recipe includes several stages. Users can start form or stop at any stage. For example, the following command achieves starting from the third stage and stopping at the fifth stage:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>. ./run.sh --stage <span class="m">3</span> --stop_stage <span class="m">5</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Training Steps
FunASR supports two parameters to specify the training steps, namely <code class="docutils literal notranslate"><span class="pre">max_epoch</span></code> and <code class="docutils literal notranslate"><span class="pre">max_update</span></code>. <code class="docutils literal notranslate"><span class="pre">max_epoch</span></code> indicates the total training epochs while <code class="docutils literal notranslate"><span class="pre">max_update</span></code> indicates the total training steps. If these two parameters are specified at the same time, once the training reaches any one of these two parameters, the training will be stopped.</p></li>
<li><p>Change the configuration of the model</p></li>
</ul>
<p>The configuration of the model is set in the config file <code class="docutils literal notranslate"><span class="pre">conf/train_*.yaml</span></code>. Specifically, the default encoder configuration of paraformer is as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">encoder</span><span class="p">:</span> <span class="n">conformer</span>
<span class="n">encoder_conf</span><span class="p">:</span>
    <span class="n">output_size</span><span class="p">:</span> <span class="mi">256</span>    <span class="c1"># dimension of attention</span>
    <span class="n">attention_heads</span><span class="p">:</span> <span class="mi">4</span>  <span class="c1"># the number of heads in multi-head attention</span>
    <span class="n">linear_units</span><span class="p">:</span> <span class="mi">2048</span>  <span class="c1"># the number of units of position-wise feed forward</span>
    <span class="n">num_blocks</span><span class="p">:</span> <span class="mi">12</span>      <span class="c1"># the number of encoder blocks</span>
    <span class="n">dropout_rate</span><span class="p">:</span> <span class="mf">0.1</span>
    <span class="n">positional_dropout_rate</span><span class="p">:</span> <span class="mf">0.1</span>
    <span class="n">attention_dropout_rate</span><span class="p">:</span> <span class="mf">0.0</span>
    <span class="n">input_layer</span><span class="p">:</span> <span class="n">conv2d</span>  <span class="c1"># encoder input layer architecture type</span>
    <span class="n">normalize_before</span><span class="p">:</span> <span class="n">true</span>
    <span class="n">pos_enc_layer_type</span><span class="p">:</span> <span class="n">rel_pos</span>
    <span class="n">selfattention_layer_type</span><span class="p">:</span> <span class="n">rel_selfattn</span>
    <span class="n">activation_type</span><span class="p">:</span> <span class="n">swish</span>
    <span class="n">macaron_style</span><span class="p">:</span> <span class="n">true</span>
    <span class="n">use_cnn_module</span><span class="p">:</span> <span class="n">true</span>
    <span class="n">cnn_module_kernel</span><span class="p">:</span> <span class="mi">15</span>
</pre></div>
</div>
<p>Users can change the encoder configuration by modify these values. For example, if users want to use an encoder with 16 conformer blocks and each block has 8 attention heads, users just need to change <code class="docutils literal notranslate"><span class="pre">num_blocks</span></code> from 12 to 16 and change <code class="docutils literal notranslate"><span class="pre">attention_heads</span></code> from 4 to 8. Besides, the batch_size, learning rate and other training hyper-parameters are also set in this config file. To change these hyper-parameters, users just need to directly change the corresponding values in this file. For example, the default learning rate is <code class="docutils literal notranslate"><span class="pre">0.0005</span></code>. If users want to change the learning rate to 0.0002, set the value of lr as <code class="docutils literal notranslate"><span class="pre">lr:</span> <span class="pre">0.0002</span></code>.</p>
<ul class="simple">
<li><p>Decoding by CPU or GPU</p></li>
</ul>
<p>We support CPU and GPU decoding. For CPU decoding, set <code class="docutils literal notranslate"><span class="pre">gpu_inference=false</span></code> and <code class="docutils literal notranslate"><span class="pre">njob</span></code> to specific the total number of CPU jobs. For GPU decoding, first set <code class="docutils literal notranslate"><span class="pre">gpu_inference=true</span></code>. Then set <code class="docutils literal notranslate"><span class="pre">gpuid_list</span></code> to specific which GPUs for decoding and <code class="docutils literal notranslate"><span class="pre">njob</span></code> to specific the number of decoding jobs on each GPU.</p>
</div>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../installation/docker.html" class="btn btn-neutral float-left" title="Docker" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="punc_recipe.html" class="btn btn-neutral float-right" title="Punctuation Restoration" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Speech Lab, Alibaba Group.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>