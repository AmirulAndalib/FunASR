<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Track &amp; Evaluation &mdash; FunASR  documentation</title><link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            FunASR
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption"><span class="caption-text">Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation/installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation/docker.html">Docker</a></li>
</ul>
<p class="caption"><span class="caption-text">Quick Start</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../funasr/quick_start.html">Quick Start</a></li>
</ul>
<p class="caption"><span class="caption-text">Academic Egs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../academic_recipe/asr_recipe.html">Speech Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../academic_recipe/punc_recipe.html">Punctuation Restoration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../academic_recipe/vad_recipe.html">Voice Activity Detection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../academic_recipe/sv_recipe.html">Speaker Verification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../academic_recipe/sd_recipe.html">Speaker Diarization</a></li>
</ul>
<p class="caption"><span class="caption-text">ModelScope Egs</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../modelscope_pipeline/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../egs_modelscope/asr/TEMPLATE/README.html">Speech Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../egs_modelscope/vad/TEMPLATE/README.html">Voice Activity Detection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../egs_modelscope/punctuation/TEMPLATE/README.html">Punctuation Restoration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../egs_modelscope/tp/TEMPLATE/README.html">Timestamp Prediction (FA)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modelscope_pipeline/sv_pipeline.html">Speaker Verification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modelscope_pipeline/sd_pipeline.html">Speaker Diarization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../modelscope_pipeline/itn_pipeline.html">Inverse Text Normalization (ITN)</a></li>
</ul>
<p class="caption"><span class="caption-text">Model Zoo</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../model_zoo/modelscope_models.html">Pretrained Models Released on ModelScope</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_zoo/huggingface_models.html">Pretrained Models on Huggingface</a></li>
</ul>
<p class="caption"><span class="caption-text">Runtime and Service</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../runtime/readme.html">FunASR Runtime Roadmap</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime/docs/SDK_tutorial_online.html">FunASR Realtime Transcribe Service</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime/docs/SDK_tutorial.html">Highlights</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime/docs/SDK_tutorial.html#funasr-offline-file-transcription-service">FunASR Offline File Transcription Service</a></li>
<li class="toctree-l1"><a class="reference internal" href="../runtime/html5/readme.html">Speech Recognition Service Html5 Client Access Interface</a></li>
</ul>
<p class="caption"><span class="caption-text">Benchmark and Leaderboard</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../benchmark/benchmark_pipeline_cer.html">Leaderboard IO</a></li>
</ul>
<p class="caption"><span class="caption-text">Funasr Library</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../reference/build_task.html">Build custom tasks</a></li>
</ul>
<p class="caption"><span class="caption-text">Papers</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../reference/papers.html">Papers</a></li>
</ul>
<p class="caption"><span class="caption-text">Application</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../reference/application.html">Audio Cut</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/application.html#realtime-speech-recognition">Realtime Speech Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/application.html#audio-chat">Audio Chat</a></li>
</ul>
<p class="caption"><span class="caption-text">FQA</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../reference/FQA.html">FQA</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">FunASR</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Track &amp; Evaluation</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/m2met2/Track_setting_and_evaluation.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div class="section" id="track-evaluation">
<h1>Track &amp; Evaluation<a class="headerlink" href="#track-evaluation" title="Permalink to this headline"></a></h1>
<div class="section" id="speaker-attributed-asr">
<h2>Speaker-Attributed ASR<a class="headerlink" href="#speaker-attributed-asr" title="Permalink to this headline"></a></h2>
<p>The speaker-attributed ASR task poses a unique challenge of transcribing speech from multiple speakers and assigning a speaker label to the transcription. Figure 2 illustrates the difference between the speaker-attributed ASR task and the multi-speaker ASR task. This track allows for the use of the AliMeeting, Aishell4, and Cn-Celeb datasets as constrained data sources during both training and evaluation. The AliMeeting dataset, which was used in the M2MeT challenge, includes Train, Eval, and Test sets. Additionally, a new Test-2023 set, consisting of approximately 10 hours of meeting data recorded in an identical acoustic setting as the AliMeeting corpus, will be released soon for challenge scoring and ranking. It’s worth noting that the organizers will not provide the near-field audio, transcriptions, or oracle timestamps of the Test-2023 set. Instead, segments containing multiple speakers will be provided, which can be obtained using a simple voice activity detection (VAD) model.</p>
<p><img alt="task difference" src="../_images/task_diff.png" /></p>
</div>
<div class="section" id="evaluation-metric">
<h2>Evaluation metric<a class="headerlink" href="#evaluation-metric" title="Permalink to this headline"></a></h2>
<p>The accuracy of a speaker-attributed ASR system is evaluated using the concatenated minimum permutation character error rate (cpCER) metric. The calculation of cpCER involves three steps. Firstly, the reference and hypothesis transcriptions from each speaker in a session are concatenated in chronological order. Secondly, the character error rate (CER) is calculated between the concatenated reference and hypothesis transcriptions, and this process is repeated for all possible speaker permutations. Finally, the permutation with the lowest CER is selected as the cpCER for that session. TThe CER is obtained by dividing the total number of insertions (Ins), substitutions (Sub), and deletions(Del) of characters required to transform the ASR output into the reference transcript by the total number of characters in the reference transcript. Specifically, CER is calculated by:</p>
<p>$$\text{CER} = \frac {\mathcal N_{\text{Ins}} + \mathcal N_{\text{Sub}} + \mathcal N_{\text{Del}} }{\mathcal N_{\text{Total}}} \times 100%,$$</p>
<p>where $\mathcal N_{\text{Ins}}$, $\mathcal N_{\text{Sub}}$, $\mathcal N_{\text{Del}}$ are the character number of the three errors, and $\mathcal N_{\text{Total}}$ is the total number of characters.</p>
</div>
<div class="section" id="sub-track-arrangement">
<h2>Sub-track arrangement<a class="headerlink" href="#sub-track-arrangement" title="Permalink to this headline"></a></h2>
<div class="section" id="sub-track-i-fixed-training-condition">
<h3>Sub-track I (Fixed Training Condition):<a class="headerlink" href="#sub-track-i-fixed-training-condition" title="Permalink to this headline"></a></h3>
<p>Participants are required to use only the fixed-constrained data (i.e., AliMeeting, Aishell-4, and CN-Celeb) for system development. The usage of any additional data is strictly prohibited. However, participants can use open-source pre-trained models from third-party sources, such as <a class="reference external" href="https://huggingface.co/models">Hugging Face</a> and <a class="reference external" href="https://www.modelscope.cn/models">ModelScope</a>, provided that the list of utilized models is clearly stated in the final system description paper.</p>
</div>
<div class="section" id="sub-track-ii-open-training-condition">
<h3>Sub-track II (Open Training Condition):<a class="headerlink" href="#sub-track-ii-open-training-condition" title="Permalink to this headline"></a></h3>
<p>Participants are allowed to use any publicly available data set, privately recorded data, and manual simulation to build their systems in addition to the fixed-constrained data. They can also utilize any open-source pre-trained models, but it is mandatory to provide a clear list of the data and models used in the final system description paper. If manually simulated data is used, a detailed description of the data simulation scheme must be provided.</p>
</div>
</div>
</div>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Speech Lab, Alibaba Group.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>